---
title: "DATA 621 HW 1 draft"
author: "Group 1"
date: "9/20/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## To do

Analysis of residuals plots for each model (or maybe just optimal model)
Clean up viz
Model selection section (All three of our models performed very similarly. The third model is the simplest, and that's the one I think we should use.)

## Data Exploration

```{r}
library(mice)
library(tidyverse)
library(GGally)
library(psych)
library(stats)
library(corrplot)

library(cowplot)
library(magrittr)
library(skimr)
library(DataExplorer)
library(caret)
library(MASS)
library(regclass)
library(moderndive)

set.seed(210904)

raw <- read.csv("https://raw.githubusercontent.com/sconnin/621_BusinessAnalytics/HW1_MoneyBall/moneyball-training-data.csv")
```

```{r}
summary(raw)
```

Here we can see that some variables show a significant number of missing values, especially `TEAM_BATTING_HBP` and `TEAM_BASERUN_CS`. Some variables show suspiciously large maximum values, such as `TEAM_PITCHING_H`, `TEAM_PITCHING_BB`, `TEAM_PITCHING_SO`, and `TEAM_FIELDING_E`. We can also see that some variables contain entries of zero that don't make sense in the context of a baseball season. These variables include `TARGET_WINS`, `TEAM_BATTING_3B`, `TEAM_BATTING_HR`, `TEAM_BATTING_BB`, `TEAM_BATTING_SO`, `TEAM_BASERUN_SB`, `TEAM_BASERUN_CS`, `TEAM_PITCHING_HR`, `TEAM_PITCHING_BB`, `TEAM_PITCHING_SO`. At least some of these entries we know to be erroneous. For example, the [all-time minimum](https://www.baseball-almanac.com/recbooks/rb_strike2.shtml) batting strikeouts for a team over a complete season was 308, achieved by the 1921 Cincinnati Reds.  

Investigating NAs more closely:

```{r}
#colMeans(is.na(raw))
plot_missing(raw)
```

`TEAM_BATTING_HBP` is comprised of almost 92% missing values. This variable cannot provide much information to our model. `TEAM_BASERUN`CS` also displays a very high fraction of missing values: 34%.  

Our findings so far mean that we will need a strategy to mitigate the impact of potentially erroneous outliers, and we will need to respond to the missingness in the data set.  

Examining the shape of each variable:  

```{r}
raw %>%
  gather() %>%
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = "free") +
  geom_density()
```

Density plots for each variable reveals the highly skewed shapes of `TEAM_FIELDING_E`, `TEAM_PITCHING_BB`, `TEAM_PITCHING_H`, AND `TEAM_PITCHING_SO` as suggested by the numeric summary above. `TEAM_BASERUN_SB` and `TEAM_BATTING_3B` display moderate skewness. Other distributions appear roughly normal or bimodal. Let's look more closely at each of the highly skewed variables:  

```{r}
ggplot(data = raw, aes(x = TEAM_FIELDING_E)) +
  geom_histogram()
```

A very small number of entries exceed 1000. Excluding these, the distribution is moderately skewed right.  

```{r}
ggplot(data = raw, aes(x = TEAM_PITCHING_BB)) +
  geom_histogram()
```

The distribution appears normal except for a very small number of entries greater than 1000.  

```{r}
ggplot(data = raw, aes(x = TEAM_PITCHING_H)) +
  geom_histogram()
```

Let's look more closely at the region where most of this variable's values lie, [0,6000]:

```{r}
raw %>%
  filter(TEAM_PITCHING_H < 6000) %>%
  ggplot(aes(x = TEAM_PITCHING_H)) +
  geom_histogram()
```

The variable is skewed even within this narrower region surrounding the peak. This suggests possible data entry error for values greater than 3250.

```{r}
ggplot(data = raw, aes(x = TEAM_PITCHING_SO)) +
  geom_histogram()
```

Narrowing in:

```{r}
raw %>%
  filter(TEAM_PITCHING_SO < 2500) %>%
  ggplot(aes(x = TEAM_PITCHING_SO)) +
  geom_histogram()
```

The data appear roughly normal, except for a significant number of unrealistic zero-values. This suggests possible data entry error for values of zero, or greater than or equal to 2000.

```{r}
ggplot(data = raw, aes(x = TEAM_BASERUN_SB)) +
  geom_histogram()
```

The shape of this data does not suggest any data entry errors, even though there exist a few extreme outliers.

```{r}
ggplot(data = raw, aes(x = TEAM_BATTING_3B)) +
  geom_histogram()
```

The shape of this data does not suggest any data entry errors, even though there exist a few extreme outliers.

An important condition of linear modeling is that variables be uncorrelated with each other. In real-world data, it is rarely possible to fully satisfy this condition. However, we can seek to avoid large pairwise correlations between variables.  

The correlation plot below is arranged so that all variables with a theoretical positive effect on `TARGET_WINS` appear first, followed by variables with a theoretical negative effect. We would expect positive correlations among variables with positive effects, positive correlations among variables with negative effects, and negative correlations among variables with opposite effects.  

```{r}
#reorder based on variable list on assignment sheet. Drop TEAM_BATTING_HBP and TEAM_BASERUN_CS due to missingness.
tmp <- raw[,c(2:7,9,17,15,8,16,14,12,13)]
correlation <- cor(tmp, use = "complete.obs")
corrplot.mixed(correlation, tl.col = 'black', tl.pos = 'lt')
```

But this is not what we see. Instead we see that no variable alone, has correlation greater than 0.35 in either direction with the target. And we also see that some pairs of variables have correlations that are so strong or misdirected that we have reason to doubt the integrity of the data. Note that `TEAM_PITCHING_HR` and `TEAM_BATTING_HR`, with correlation 0.98, contain essentially the same data.  

Another assumption of linear models is that explanatory variables are linearly related to the response variable. Let's examine this using scatterplots. First, we examine the batting variables:  

```{r}
raw %>%
  gather(starts_with("TEAM_BAT"), key = "var", value = "value") %>%
  ggplot(aes(x = value, y = TARGET_WINS)) +
  geom_point() +
  facet_wrap(~ var, scales = "free")
```

###
What can we say about whether these explanatory variables are linearly related to TARGET_WINS? It doesn't seem like there's much of a relationship of any kind, either linear or nonlinear.
###

Examining `BASERUN` and `FIELDING` variables:

```{r}
raw %>%
  gather(c(starts_with("TEAM_BASERUN"), starts_with("TEAM_FIELD")), key = "var", value = "value") %>%
  ggplot(aes(x = value, y = TARGET_WINS)) +
  geom_point() +
  facet_wrap(~ var, scales = "free")
```

###
What can we say about these?
###

Examining `PITCHING` variables:  

```{r}
raw %>%
  gather(starts_with("TEAM_PITCH"), key = "var", value = "value") %>%
  ggplot(aes(x = value, y = TARGET_WINS)) +
  geom_point() +
  facet_wrap(~ var, scales = "free")
```

###
Gross.
###

An unexpected finding in our exploratory data analysis is that a scatterplot of `TEAM_BATTING_SO` and `TEAM_PITCHING_SO` suggests that the data can be divided into 4 distinct groups, three of which contain highly correlated data:

```{r}
raw %>%
  mutate(SO_factor = case_when(TEAM_BATTING_SO >= TEAM_PITCHING_SO*.96+10~ 'high',
                              (TEAM_BATTING_SO<TEAM_PITCHING_SO*.96+10 & TEAM_BATTING_SO>TEAM_PITCHING_SO*.96-50) ~'med_high',
                              (TEAM_BATTING_SO<TEAM_PITCHING_SO*.96-50 & TEAM_BATTING_SO>TEAM_PITCHING_SO*.96-120) ~'med_low',
                              TEAM_BATTING_SO<TEAM_PITCHING_SO*.96-120 ~'low')) %>%
  filter(TEAM_PITCHING_SO < 2000) %>%
  ggplot(aes(x = TEAM_PITCHING_SO, y = TEAM_BATTING_SO, colour = SO_factor)) +
  geom_point()
```

There is no theoretical reason to expect such a grouping, or to expect such high correlations between the number of strikeouts a team incurs while batting, and the number of strikeouts a team achieves while pitching. However, it may be useful to divide the data into the four groups suggested by these relationships for modeling purposes.  

## Data Preparation

*Simplify column names*.

```{r}
names(raw) <- gsub('TEAM_', '', x = names(raw))
```

*Add group variable,* `SO_FACTOR`.

```{r}
raw <- raw %>% mutate(SO_FACTOR = case_when(BATTING_SO >= PITCHING_SO*.96+10 ~ 'high',
                                           (BATTING_SO<PITCHING_SO*.96+10 & BATTING_SO>PITCHING_SO*.96-50) ~ 'med_high',
                                           (BATTING_SO<PITCHING_SO*.96-50 & BATTING_SO>PITCHING_SO*.96-120) ~ 'med_low',
                                           BATTING_SO<PITCHING_SO*.96-120 ~ 'low'))
```


*Combine variables*.

Here we construct new variables to reduce the number of pairs of correlated variables, and to better account for the structure of a baseball season.

```{r}
raw <- raw %>%
  mutate("BASERUN_NET_SB" = BASERUN_SB - BASERUN_CS) %>%
  mutate("OFFENSE_OBP" = (BATTING_H + BATTING_BB)/(BATTING_H + BATTING_BB - BASERUN_CS + (162*27))) %>%
  mutate("DEFENSE_OBP" = (PITCHING_H + FIELDING_E + PITCHING_BB - FIELDING_DP)/(PITCHING_H + FIELDING_E + PITCHING_BB - FIELDING_DP + (162*27))) %>%
  mutate("TOT_AT_BATS" = BATTING_H + BATTING_BB - BASERUN_CS + (162*27))
```

*Drop unneeded columns and move response to final column*.

```{r}

raw <- raw[,c(4:6,8,13,15,18:22,2)]
```

*Train/test split*

```{r}
train_rows <- sample(nrow(raw), 0.80 * nrow(raw), replace = FALSE)
train <- raw[train_rows,]
test <- raw[-train_rows,]
```

During our exploration of the data, we uncovered several variables with extreme outliers. Based on our knowledge of baseball, we believe these outliers may be the result of data entry error.  

To protect against possible data-entry error, we'll construct two sets of models, one that includes extreme values, and one that replaces extreme values with imputed values generated by the `mice` package. We'll consider all these models before making our final selection. 

```{r}
#training set which only missing values will be imputed
train_imp_M <- train

#training set in which missing values and outliers will be imputed
train_imp_OM <- train
```

*Impute only missing on training data*

```{r}
#Function so set the upper and lower bounds of a dataframe vector
bounds <- function(vector,upper_pct,lwr_pct){
  ub <- quantile(vector, upper_pct,na.rm = T)
  lb <- quantile(vector, lwr_pct, na.rm = T)
  
  return(c(ub,lb))
}

# I set the upper and lower quantiles here, we can discuss this further or decide that this split makes sense.
up_l <- 0.97
lo_l <- 0.03
```

```{r}
imp <- mice(train_imp_M, method = "norm.nob", m = 1)
train_imp_M <- complete(imp)

#Sean used norm.boot method and m = 5. Any thoughts on which is a better match for our data?

train_imp_M$SO_FACTOR[is.na(train_imp_M$SO_FACTOR)] <- "high"
```

*Impute missing and outliers on training data*

```{r}
colnames <- c('BATTING_2B', 'BATTING_3B', 'BATTING_HR', 'BATTING_SO', 'PITCHING_HR', 'PITCHING_SO', 'BASERUN_NET_SB', 'OFFENSE_OBP', 'DEFENSE_OBP', 'TOT_AT_BATS')

for(col in colnames){
  upper = bounds(train_imp_OM[,col],up_l,lo_l)[1]
  lower = bounds(train_imp_OM[,col],up_l,lo_l)[2]

  train_imp_OM[,col] = ifelse(train_imp_OM[,col] < lower | train_imp_OM[,col] > upper, NA, train_imp_OM[,col])
}

# Impute to replace all previously missing values along with removed outliers
imp <- mice(train_imp_OM, method = "norm.nob", m = 1)
train_imp_OM <- complete(imp)
train_imp_OM$SO_FACTOR[is.na(train_imp_OM$SO_FACTOR)] <- "high"
```

*Impute only missing on test data*

```{r}
test_imp_M <- test
imp <- mice(test_imp_M, method = "norm.nob", m = 1)
test_imp_M <- complete(imp)

test_imp_M$SO_FACTOR[is.na(test_imp_M$SO_FACTOR)] <- "high"
```

*Impute missing and outliers on test data*

```{r}
test_imp_OM <- test

for(col in colnames){
  upper = bounds(test_imp_OM[,col],up_l,lo_l)[1]
  lower = bounds(test_imp_OM[,col],up_l,lo_l)[2]

  test_imp_OM[,col] = ifelse(test_imp_OM[,col] < lower | test_imp_OM[,col] > upper, NA, test_imp_OM[,col])
}

# Impute to replace all previously missing values along with removed outliers
imp <- mice(test_imp_OM, method = "norm.nob", m = 1)
test_imp_OM <- complete(imp)
test_imp_OM$SO_FACTOR[is.na(test_imp_OM$SO_FACTOR)] <- "high"
```

## Build Models

We'll build models for training sets with only missing values imputed, and evaluate them with data for which only missing values are imputed. Then we'll build analogous models with outliers imputed as well. These models are tested on a test set where outliers are also replaced with imputed values.

```{r}
rmse <- function(lm, test) {
  preds <- predict(lm, test[,c(1:11)])
  errors <- test$TARGET_WINS - preds
  return(sqrt(sum(errors^2)/(nrow(test) - length(lm$coefficients) - 1)))
}
```

*Model 1. Almost all variables.*

This model contains all variables except `BATTING_HR`, `BATTING_SO`, and `TOT_AT_BATS`, because each of these variables is highly correlated with at least one other variable.

```{r}
lm1_M <- lm(TARGET_WINS ~ BATTING_2B + BATTING_3B + PITCHING_HR + PITCHING_SO + SO_FACTOR + BASERUN_NET_SB + OFFENSE_OBP + DEFENSE_OBP, data = train_imp_M)

lm1_OM <- lm(TARGET_WINS ~ BATTING_2B + BATTING_3B + PITCHING_HR + PITCHING_SO + SO_FACTOR + BASERUN_NET_SB + OFFENSE_OBP + DEFENSE_OBP, data = train_imp_OM)
summary(lm1_M)
summary(lm1_OM)
```


```{r}
plot(lm1_M)
plot(lm1_OM)
```

Residuals vs. Leverage:
* p. 90 in the red book. If an outlier falls outside some range, then your model will change significantly. Look at the book.

###
Interpret model and residual plots
Our initial look at the residuals for the first model yields what appears to be an issue with Heteroscedasticity.  It appears the variance of the residuals decreases at extreme values.  We also notice from QQ plots, a departure from the normal distribution at the extremes.

The adjacent plots, which are trained on the reconciled outliers, show improvement on the issues of the previous model.  This could be a hint that outliers are the difference maker, but we will explorer further.
###

How does this model perform on the holdout set?
###
```{r}
#RMSE on test set for lm1:

lm1_M_test_rmse <- rmse(lm1_M,test_imp_M)
lm1_M_test_rmse

lm1_OM_test_rmse <- rmse(lm1_OM,test_imp_OM)
lm1_OM_test_rmse
```

*Model 2. Piecewise by FACTOR_SO*.

The scatterplot of `BATTING_SO` vs `PITCHING_SO` suggested four groups in this data. Here we fit one model to each group.  

```{r}
#IMPUTING MISSING VALUES ONLY

low <- train_imp_M %>%
  filter(SO_FACTOR == "low") %>%
  dplyr::select(-SO_FACTOR)

med_low <- train_imp_M %>%
  filter(SO_FACTOR == "med_low") %>%
  dplyr::select(-SO_FACTOR)

med_high <- train_imp_M %>%
  filter(SO_FACTOR == "med_high") %>%
  dplyr::select(-SO_FACTOR)

high <- train_imp_M %>%
  filter(SO_FACTOR == "high") %>%
  dplyr::select(-SO_FACTOR)

lm_low_M <- lm(TARGET_WINS ~ BATTING_2B + BATTING_3B + PITCHING_HR + PITCHING_SO + BASERUN_NET_SB + OFFENSE_OBP + DEFENSE_OBP, data = low)
lm_med_low_M <- lm(TARGET_WINS ~ BATTING_2B + BATTING_3B + PITCHING_HR + PITCHING_SO + BASERUN_NET_SB + OFFENSE_OBP + DEFENSE_OBP, data = med_low)
lm_med_high_M <- lm(TARGET_WINS ~ BATTING_2B + BATTING_3B + PITCHING_HR + PITCHING_SO + BASERUN_NET_SB + OFFENSE_OBP + DEFENSE_OBP, data = med_high)
lm_high_M <- lm(TARGET_WINS ~ BATTING_2B + BATTING_3B + PITCHING_HR + PITCHING_SO + BASERUN_NET_SB + OFFENSE_OBP + DEFENSE_OBP, data = high)

summary(lm_low_M)
summary(lm_med_low_M)
summary(lm_med_high_M)
summary(lm_high_M)
```

```{r}
#IMPUTING MISSING VALUES AND OUTLIERS

low <- train_imp_OM %>%
  filter(SO_FACTOR == "low") %>%
  dplyr::select(-SO_FACTOR)

med_low <- train_imp_OM %>%
  filter(SO_FACTOR == "med_low") %>%
  dplyr::select(-SO_FACTOR)

med_high <- train_imp_OM %>%
  filter(SO_FACTOR == "med_high") %>%
  dplyr::select(-SO_FACTOR)

high <- train_imp_OM %>%
  filter(SO_FACTOR == "high") %>%
  dplyr::select(-SO_FACTOR)

lm_low_OM <- lm(TARGET_WINS ~ BATTING_2B + BATTING_3B + PITCHING_HR + PITCHING_SO + BASERUN_NET_SB + OFFENSE_OBP + DEFENSE_OBP, data = low)
lm_med_low_OM <- lm(TARGET_WINS ~ BATTING_2B + BATTING_3B + PITCHING_HR + PITCHING_SO + BASERUN_NET_SB + OFFENSE_OBP + DEFENSE_OBP, data = med_low)
lm_med_high_OM <- lm(TARGET_WINS ~ BATTING_2B + BATTING_3B + PITCHING_HR + PITCHING_SO + BASERUN_NET_SB + OFFENSE_OBP + DEFENSE_OBP, data = med_high)
lm_high_OM <- lm(TARGET_WINS ~ BATTING_2B + BATTING_3B + PITCHING_HR + PITCHING_SO + BASERUN_NET_SB + OFFENSE_OBP + DEFENSE_OBP, data = high)

summary(lm_low_OM)
summary(lm_med_low_OM)
summary(lm_med_high_OM)
summary(lm_high_OM)
```

```{r}
plot(lm_low_M)
plot(lm_med_low_M)
plot(lm_med_high_M)
plot(lm_high_M)

plot(lm_low_OM)
plot(lm_med_low_OM)
plot(lm_med_high_OM)
plot(lm_high_OM)
```

###
Interpret model and residual plots
To recap, the efforts of this 2nd model were prompted by the results of the BATTING_SO vs PITCHING_SO plot. There appears to be four distint groups, so we now can review how the model performs seperately within those groups.  We achknowlegde some issues with this approach; one being that the models differ greatly in population size, so a visual comparison may not be trustworthly.  Also, we may not have the appropriate context to make the decision to divide the data based on the relationship between these two predictors, and it could possibly lead to over fitting.  Nonetheless, it seems that there is a slight improvement in homoscedacity, but no decernable difference in normality.
###

How does this model perform on the holdout set?
###
```{r}
testlow_M <- test_imp_M %>%
  filter(SO_FACTOR == "low") %>%
  dplyr::select(-SO_FACTOR)

testmed_low_M <- test_imp_M %>%
  filter(SO_FACTOR == "med_low") %>%
  dplyr::select(-SO_FACTOR)

testmed_high_M <- test_imp_M %>%
  filter(SO_FACTOR == "med_high") %>%
  dplyr::select(-SO_FACTOR)

testhigh_M <- test_imp_M %>%
  filter(SO_FACTOR == "high") %>%
  dplyr::select(-SO_FACTOR)

testlow_OM <- test_imp_OM %>%
  filter(SO_FACTOR == "low") %>%
  dplyr::select(-SO_FACTOR)

testmed_low_OM <- test_imp_OM %>%
  filter(SO_FACTOR == "med_low") %>%
  dplyr::select(-SO_FACTOR)

testmed_high_OM <- test_imp_OM %>%
  filter(SO_FACTOR == "med_high") %>%
  dplyr::select(-SO_FACTOR)

testhigh_OM <- test_imp_OM %>%
  filter(SO_FACTOR == "high") %>%
  dplyr::select(-SO_FACTOR)

rmse_low_M <- rmse(lm_low_M,testlow_M)
rmse_med_low_M <- rmse(lm_med_low_M,testmed_low_M)
rmse_med_high_M <- rmse(lm_med_high_M,testmed_high_M)
rmse_high_M <- rmse(lm_high_M,testhigh_M)

#Overall RMSE:
rmse_total_M <- (rmse_low_M * nrow(testlow_M) + rmse_med_low_M * nrow(testmed_low_M) + rmse_med_high_M * nrow(testmed_high_M) + rmse_high_M * nrow(testhigh_M)) / nrow(test_imp_M)

rmse_low_OM <- rmse(lm_low_OM,testlow_OM)
rmse_med_low_OM <- rmse(lm_med_low_OM,testmed_low_OM)
rmse_med_high_OM <- rmse(lm_med_high_OM,testmed_high_OM)
rmse_high_OM <- rmse(lm_high_OM,testhigh_OM)

#Overall RMSE:
rmse_total_OM <- (rmse_low_OM * nrow(testlow_OM) + rmse_med_low_OM * nrow(testmed_low_OM) + rmse_med_high_OM * nrow(testmed_high_OM) + rmse_high_OM * nrow(testhigh_OM)) / nrow(test_imp_OM)
```


*Third model*

In this third model, we drop `DEFENSE_OBP` from `lm1`. Even though this variable is a statistically significant predictor of `TARGET_WINS`, its large coefficient and standard error compared to the other variables suggest it may be correlated with another variable. In fact, it has a correlation coefficient of ###VALUE### with `OFFENSE_OBP`.

```{r}
cor(train$OFFENSE_OBP, train$DEFENSE_OBP)
```

```{r}
lm3_M <- lm(TARGET_WINS ~ BATTING_2B + BATTING_3B + PITCHING_HR + PITCHING_SO + SO_FACTOR + BASERUN_NET_SB + OFFENSE_OBP, data = train_imp_M)
summary(lm3_M)

lm3_OM <- lm(TARGET_WINS ~ BATTING_2B + BATTING_3B + PITCHING_HR + PITCHING_SO + SO_FACTOR + BASERUN_NET_SB + OFFENSE_OBP, data = train_imp_OM)
summary(lm3_OM)
```

Removing this variable reduced the standard error for `OFFENSE_OBP`.
```{r}
plot(lm3_M)
plot(lm3_OM)
```

###
Interpret model and residual plots
We see a similar behavior of the residuals as the 1st model.  We can call attention again to the improvement in the residuals by the training of the data with the imputed outliers.  Not only is the shape of the residuals more homscadastic/normal, but the statistical significance of the beta values are much higher(lower p-value).
###

How does this model perform on the holdout set?
###
```{r}
lm3_M_test_rmse <- rmse(lm3_M,test_imp_M)
lm3_M_test_rmse

lm3_OM_test_rmse <- rmse(lm3_OM,test_imp_OM)
lm3_OM_test_rmse
```

*Summary of all model performance (RMSE)*

```{r}
print(lm1_M_test_rmse)
print(rmse_total_M)
print(lm3_M_test_rmse)

print(lm1_OM_test_rmse)
print(rmse_total_OM)
print(lm3_OM_test_rmse)
```

## Conclusion.

Lets recap the above analysis.
-Data Exploration:  We recognize a high probability of flawed data.  Unrelated variables were found to have near perfect correlation.  Outliers representing impossible statistics.  These instances call more suspecion on the less aggregious offenses including bi-modal distributions and non-linear X-Y correlations.  In a practical setting, this would need to be address before moving forward.

-1st Model:  Pushing forward with the suspecious data, we compromised by creating a factor that represent particular cohorts within the data.  Also, the variables were translated into more contextual calculations based on domain knowledge.

-2nd Model:  Following the steps of the 1st model; Except going further than just to using a factor to account for the cohorts; training the model seperately base on the cohort.

-3rd Model:  Recognizing the problems with overfitting of the 2nd model; we moved to improve the 1st model, conducting some feature engineering, resulting in the dropping of one of a pair of highly correlated variables.

-Model Selection:  We establish the winner to be the 3rd model.  It uses the full training set, unlike model 2.  It's predictors and the intercept are highly significant; unlike model 1.  We beleive the factors added to model serve as a good attempt to counteract the flaws in the data, and the feature engineering made the model more efficient and better performing.  Although the strategy of imputing the outliers increased rmse slightly, we determined from the improvement in the residual plots that it is the better generalized model.























